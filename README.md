# Hallucination Cases

This repository documents clear cases of factual hallucinations produced by large language models, primarily observed in Spanish-language contexts and in cultural references specific to Spain.

Most examples involve interactions with contemporary LLMs (e.g. ChatGPT and Claude), although the focus is on the error patterns rather than on any specific model.

Each case describes situations where non-existent entities, events, or facts are invented and presented with high confidence, without appropriate uncertainty, verification, or clarification.

The goal is to provide concrete, well-documented examples for qualitative evaluation, linguistic QA analysis, and model auditing.

